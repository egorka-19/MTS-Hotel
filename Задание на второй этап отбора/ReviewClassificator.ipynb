{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Иницилизация всех библиотек**"
      ],
      "metadata": {
        "id": "au0vAmikltAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZVz-OS3C5mBL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install translate"
      ],
      "metadata": {
        "id": "ryYep86EFQtc",
        "outputId": "262f1339-b2f3-4ebc-bacd-cc8ba0d4d455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting translate\n",
            "  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from translate) (8.1.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from translate) (5.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from translate) (2.32.3)\n",
            "Collecting libretranslatepy==2.1.1 (from translate)\n",
            "  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->translate) (2024.8.30)\n",
            "Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n",
            "Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n",
            "Installing collected packages: libretranslatepy, translate\n",
            "Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Создание кастомного датасета, для правильности ввода модели**"
      ],
      "metadata": {
        "id": "bbUcwrwAmKvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, texts, targets, tokenizer, max_len=512):\n",
        "    self.texts = texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    target = self.targets[idx]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "metadata": {
        "id": "caD4BU4N7cyN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация текстовой модели **BERT**, в конструкторе которой указывается кол-во классов, число эпох, предобученная модель и модель токениризации\n"
      ],
      "metadata": {
        "id": "SQ7IBK0vmZH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier:\n",
        "\n",
        "    def __init__(self, model_path, tokenizer_path, n_classes=2, epochs=3, model_save_path='/content/bert.pt'):\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_save_path=model_save_path\n",
        "        self.max_len = 512\n",
        "        self.epochs = epochs\n",
        "        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
        "        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
        "        # create datasets\n",
        "        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n",
        "        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n",
        "\n",
        "        # create data loaders\n",
        "        self.train_loader = DataLoader(self.train_set, batch_size=64, shuffle=True)\n",
        "        self.valid_loader = DataLoader(self.valid_set, batch_size=64, shuffle=True)\n",
        "\n",
        "        # helpers initialization\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "                self.optimizer,\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=len(self.train_loader) * self.epochs\n",
        "            )\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    def fit(self):\n",
        "        self.model = self.model.train()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for data in self.train_loader:\n",
        "            input_ids = data[\"input_ids\"].to(self.device)\n",
        "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "            targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = self.loss_fn(outputs.logits, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        train_acc = correct_predictions.double() / len(self.train_set)\n",
        "        train_loss = np.mean(losses)\n",
        "        return train_acc, train_loss\n",
        "\n",
        "    def eval(self):\n",
        "        self.model = self.model.eval()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.valid_loader:\n",
        "                input_ids = data[\"input_ids\"].to(self.device)\n",
        "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "                targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                loss = self.loss_fn(outputs.logits, targets)\n",
        "                correct_predictions += torch.sum(preds == targets)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "        val_acc = correct_predictions.double() / len(self.valid_set)\n",
        "        val_loss = np.mean(losses)\n",
        "        return val_acc, val_loss\n",
        "\n",
        "    def train(self):\n",
        "        best_accuracy = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
        "            train_acc, train_loss = self.fit()\n",
        "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "            val_acc, val_loss = self.eval()\n",
        "            print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            if val_acc > best_accuracy:\n",
        "                torch.save(self.model, self.model_save_path)\n",
        "                best_accuracy = val_acc\n",
        "\n",
        "        self.model = torch.load(self.model_save_path)\n",
        "\n",
        "    def predict(self, text):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        out = {\n",
        "              'text': text,\n",
        "              'input_ids': encoding['input_ids'].flatten(),\n",
        "              'attention_mask': encoding['attention_mask'].flatten()\n",
        "          }\n",
        "\n",
        "        input_ids = out[\"input_ids\"].to(self.device)\n",
        "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask.unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "02j78Cr0AER_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка ***датасета***"
      ],
      "metadata": {
        "id": "iFbOJKqqnGy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data.csv')\n"
      ],
      "metadata": {
        "id": "GwvhBUAT7q4y"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод информации, чтобы посмотреть размерность и название классов"
      ],
      "metadata": {
        "id": "mZnrBHaqnPGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvRHcqWf9yoZ",
        "outputId": "539480e1-280f-426a-deb4-cbfb6beefd11"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Кол-во** классов"
      ],
      "metadata": {
        "id": "JUWEC2jnnVax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Rating'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "RZzNSfbHCv3z",
        "outputId": "d25d26ba-366e-48e4-a674-adcda4923a9f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Rating\n",
              "5    9054\n",
              "4    6039\n",
              "3    2184\n",
              "2    1793\n",
              "1    1421\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rating</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение датасета на выборку **train** и **val**"
      ],
      "metadata": {
        "id": "E3rI77PAndYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = df[:12000]\n",
        "val_data = df[12000:17000]\n",
        "test_data = df[17000:17501]"
      ],
      "metadata": {
        "id": "1ML4E62B9jaE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка **модели**"
      ],
      "metadata": {
        "id": "z-QdHgXCnrln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = BertClassifier(\n",
        "        model_path='cointegrated/rubert-tiny',\n",
        "        tokenizer_path='cointegrated/rubert-tiny',\n",
        "        n_classes=6,\n",
        "        epochs=10,\n",
        "        model_save_path='/content/bert.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prgFBL9u-xpi",
        "outputId": "dce4fdd7-65fb-42e3-b0ae-e6956b3b6e56"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройка **токениризатора**"
      ],
      "metadata": {
        "id": "08idY-HGnykk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.preparation(\n",
        "        X_train=list(train_data['Review']),\n",
        "        y_train=list(train_data['Rating']),\n",
        "        X_valid=list(val_data['Review']),\n",
        "        y_valid=list(val_data['Rating'])\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBEMABlh_MN8",
        "outputId": "0c1fad51-2092-4088-f7c6-f1376fcbfefb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение **модели**"
      ],
      "metadata": {
        "id": "LNyZx9GFn3o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LiqMbgo_ZL2",
        "outputId": "955b16c5-1733-4c4c-c769-c7b72434c26e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train loss 1.047251488934172 accuracy 0.54375\n",
            "Val loss 0.8675375905218004 accuracy 0.6204000000000001\n",
            "----------\n",
            "Epoch 2/10\n",
            "Train loss 0.8352451749304508 accuracy 0.6365\n",
            "Val loss 0.7996029438851755 accuracy 0.6584\n",
            "----------\n",
            "Epoch 3/10\n",
            "Train loss 0.7793711401680683 accuracy 0.66125\n",
            "Val loss 0.7878778773018077 accuracy 0.662\n",
            "----------\n",
            "Epoch 4/10\n",
            "Train loss 0.7434933889419475 accuracy 0.6845\n",
            "Val loss 0.790104290352592 accuracy 0.6648000000000001\n",
            "----------\n",
            "Epoch 5/10\n",
            "Train loss 0.7174855058497571 accuracy 0.69025\n",
            "Val loss 0.7825516718852369 accuracy 0.665\n",
            "----------\n",
            "Epoch 6/10\n",
            "Train loss 0.6942767673667442 accuracy 0.7035833333333333\n",
            "Val loss 0.7906637508657914 accuracy 0.6616000000000001\n",
            "----------\n",
            "Epoch 7/10\n",
            "Train loss 0.6789907193247308 accuracy 0.7108333333333333\n",
            "Val loss 0.7785571563658835 accuracy 0.6672\n",
            "----------\n",
            "Epoch 8/10\n",
            "Train loss 0.6698125426439528 accuracy 0.7195\n",
            "Val loss 0.7909971807576432 accuracy 0.6674\n",
            "----------\n",
            "Epoch 9/10\n",
            "Train loss 0.6570026349831135 accuracy 0.7260833333333333\n",
            "Val loss 0.7826582510260087 accuracy 0.6674\n",
            "----------\n",
            "Epoch 10/10\n",
            "Train loss 0.6498941441482686 accuracy 0.7291666666666666\n",
            "Val loss 0.7944516030293477 accuracy 0.6638000000000001\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-498677ae38db>:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model = torch.load(self.model_save_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение на объекты и целевую переменную датафрейма **test** для тестирование качества модели"
      ],
      "metadata": {
        "id": "pJMwPqidoEgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = list(test_data['Review'])\n",
        "rating = list(test_data['Rating'])"
      ],
      "metadata": {
        "id": "hSKGlZZQVn2R"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись предсказаний модели в массив, для определения среднего значения рейтинга отеля"
      ],
      "metadata": {
        "id": "IMtpIrmAogpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "\n",
        "for txt in review:\n",
        "    predictions = classifier.predict(txt)\n",
        "    preds.append(predictions)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_LElyehJWDQU"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Демонстрация работы модели путем сравнение рейтинга на основе среднего значения **тестового** датасета и среднего значения предсказаний модели"
      ],
      "metadata": {
        "id": "lBYS-qLepWqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(rating), np.mean(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELMwKleifMBk",
        "outputId": "81a902c4-5f24-4365-c13f-866df763edc4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.06187624750499, 4.043912175648702)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так как для демонстрации работы модели использовался готовый датасет с отзывами на английском языке, потому что языковые модели лучше его воспринимают\n",
        ", а отзывы на российских сайтах соответственно на русском, для решения этой проблемы предлагается данный алгоритм:"
      ],
      "metadata": {
        "id": "0X3jtP8eotLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from translate import Translator\n",
        "translator= Translator(from_lang=\"russian\", to_lang=\"english\")\n",
        "translation = translator.translate(\"your text\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "k3OAgnCBpGBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}